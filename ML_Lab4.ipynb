{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42db2837-3a35-4973-851f-f2118bf0c574",
   "metadata": {},
   "source": [
    "**Practical-4**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dc99a-ad5a-4ff4-a195-4d66d37a336f",
   "metadata": {},
   "source": [
    "**Aim: Linear Regression with Regularization (without using sklearn or equivalent library) and Simple and Multiple Linear Regression with and without regularization using Sklearn \n",
    "Apply it on datasets used in experiment 3.  \n",
    "Compare outcome of experiment 3 and 4 and derive conclusions.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac090cbd-77f2-4ca0-a8f3-676081be2276",
   "metadata": {},
   "source": [
    "**Batch Gradient Descant, Stochastic Gradient Descant and Mini Batch Gradient Descant with Regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04894ae6-1a16-4089-8acc-c146ee53cbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bc25031-1bc6-49ba-af0d-f330463e6dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose which Linear Regression you want to go for:\n",
      "1. Simple Linear Regression\n",
      "2. Multiple Linear Regression\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Option: 1\n",
      "X shape: (100, 1)\n",
      "y shape: (100, 1)\n",
      "True Theta:\n",
      " [[4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Choose which Linear Regression you want to go for:\")\n",
    "print(\"1. Simple Linear Regression\")\n",
    "print(\"2. Multiple Linear Regression\")\n",
    "\n",
    "def switch_example(value):\n",
    "    if value == 1:\n",
    "        # Simple Linear Regression: y = 4 + 3X + noise\n",
    "        X = 2 * np.random.rand(100, 1)\n",
    "        true_theta_simple = np.array([4, 3]).reshape(-1, 1)\n",
    "        y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "        return (X, y, true_theta_simple)\n",
    "\n",
    "    elif value == 2:\n",
    "        # Multiple Linear Regression: y = 4 + 3x1 + 3x2 + 3x3 + noise\n",
    "        X = 2 * np.random.rand(100, 3)\n",
    "        true_theta_multiple = np.array([4, 3, 3, 3]).reshape(-1, 1)\n",
    "        y = 4 + 3*X[:, 0] + 3*X[:, 1] + 3*X[:, 2] + np.random.randn(100)\n",
    "        y = y.reshape(-1, 1)\n",
    "        return (X, y, true_theta_multiple)\n",
    "\n",
    "    else:\n",
    "        return \"Invalid choice. Please select 1 or 2.\"\n",
    "\n",
    "# Get user input\n",
    "choice = int(input(\"Enter 1 or 2: \"))\n",
    "\n",
    "result = switch_example(choice)\n",
    "\n",
    "if isinstance(result, tuple):\n",
    "    X, y, true_theta = result\n",
    "    print(\"\\nSelected Option:\", choice)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"True Theta:\\n\", true_theta)\n",
    "else:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd5ccca4-6756-4d98-923e-3299034e5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26272097-616e-437b-a627-73fe860968da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split into train (80) and test (20) ---\n",
    "split_idx = int(0.8 * m)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1866baa1-65ed-4293-8154-cbbfdf54f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term\n",
    "X_train_b = np.c_[np.ones((len(X_train), 1)), X_train]  # (80, 4)\n",
    "X_test_b = np.c_[np.ones((len(X_test), 1)), X_test]    # (20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff4846f-d541-4038-866f-ed30680230e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "class GDRegressor:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, epochs=100, l1_lambda=0.01,use_sklearn=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.use_sklearn = use_sklearn\n",
    "        self.theta = None\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        if self.use_sklearn:\n",
    "            # Use scikit-learn's Lasso\n",
    "            self.model = Lasso(alpha=self.l1_lambda, max_iter=10000)\n",
    "            self.model.fit(X_train, y_train.ravel())\n",
    "            self.theta = np.r_[self.model.intercept_, self.model.coef_]  # combine bias + weights\n",
    "            print(\"\\n[scikit-learn] Theta:\", self.theta)\n",
    "        \n",
    "        else:\n",
    "            # Initialize theta\n",
    "            self.theta = np.random.randn(X_train.shape[1], 1)\n",
    "            \n",
    "            for i in range(self.epochs):\n",
    "                y_pred_train = X_train.dot(self.theta)\n",
    "                \n",
    "                # Gradient of MSE loss\n",
    "                g = (2 / len(y_train)) * X_train.T.dot(y_pred_train - y_train)\n",
    "                \n",
    "                # Add L1 regularization term (subgradient)\n",
    "                l1_penalty = self.l1_lambda * np.sign(self.theta)\n",
    "                \n",
    "                # Update theta\n",
    "                self.theta = self.theta - self.learning_rate * (g + l1_penalty)\n",
    "                \n",
    "            print(\"\\nPredicted Theta:\\n\", self.theta)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        if self.use_sklearn:\n",
    "            return self.model.predict(X_test)\n",
    "        else:\n",
    "            return X_test.dot(self.theta)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c708fd9-5e30-4a45-b52a-437c199046f6",
   "metadata": {},
   "source": [
    "**With Regularization using Sklearn as well without using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9d6b9d-95eb-4860-81d8-d280d0b73569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Theta:\n",
      " [[3.6676112 ]\n",
      " [3.22877455]]\n",
      "\n",
      "[scikit-learn] Theta: [3.70182767 3.19940434]\n",
      "MSE (Custom GD with L1): 0.899613940591393\n",
      "MSE (scikit-learn Lasso): 0.8916619491384082\n"
     ]
    }
   ],
   "source": [
    "# With your gradient descent\n",
    "gd_model = GDRegressor(learning_rate=0.1, epochs=200, l1_lambda=0.01, use_sklearn=False)\n",
    "gd_model.fit(X_train_b, y_train)   \n",
    "y_pred_gd = gd_model.predict(X_test_b)\n",
    "\n",
    "# With scikit-learn’s Lasso\n",
    "sk_model = GDRegressor(l1_lambda=0.01, use_sklearn=True)\n",
    "sk_model.fit(X_train, y_train)     \n",
    "y_pred_sk = sk_model.predict(X_test)\n",
    "\n",
    "\n",
    "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
    "mse_sk = mean_squared_error(y_test, y_pred_sk)\n",
    "\n",
    "print(\"MSE (Custom GD with L1):\", mse_gd)\n",
    "print(\"MSE (scikit-learn Lasso):\", mse_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f08ac578-bf05-4a57-8082-331b3410aae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True theta:\n",
      " [[4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"True theta:\\n\", true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "345a4fcc-5394-4001-a606-c310087da08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor as SkSGD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "class SGDRegressor:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, epochs=100, lambda_reg=0.1, use_sklearn=False):\n",
    "        self.learning_rate = learning_rate   \n",
    "        self.epochs = epochs\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.theta = None\n",
    "        self.model = None\n",
    "        self.use_sklearn = use_sklearn\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        if self.use_sklearn:\n",
    "            # Use scikit-learn’s SGDRegressor with L2 regularization\n",
    "            self.model = SkSGD(\n",
    "                penalty=\"l2\",\n",
    "                alpha=self.lambda_reg,         # regularization strength\n",
    "                max_iter=self.epochs,\n",
    "                eta0=self.learning_rate,\n",
    "                learning_rate=\"invscaling\",      \n",
    "                fit_intercept=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            self.model.fit(X_train, y_train.ravel())\n",
    "\n",
    "            # Combine intercept + coefficients to match \"theta\"\n",
    "            self.theta = np.r_[self.model.intercept_, self.model.coef_].reshape(-1, 1)\n",
    "            print(\"\\n[scikit-learn Ridge/SGD] Theta:\\n\", self.theta)\n",
    "\n",
    "        else:\n",
    "            # Initialize theta\n",
    "                self.theta = np.random.randn(X_train.shape[1], 1)\n",
    "                \n",
    "                for i in range(self.epochs):\n",
    "                    for j in range(X_train.shape[0]):\n",
    "                        \n",
    "                        idx = np.random.randint(0, X_train.shape[0])\n",
    "                        \n",
    "                        X_row = X_train[idx: idx + 1]\n",
    "                        y_row = y_train[idx: idx + 1]\n",
    "        \n",
    "                        y_pred = X_row.dot(self.theta)\n",
    "                        \n",
    "                        # Gradient of MSE loss\n",
    "                        g = 2 * X_row.T.dot(y_pred - y_row)\n",
    "        \n",
    "                        # Ridge penalty (L2)\n",
    "                        ridge_penalty = 2 * self.lambda_reg * self.theta\n",
    "                        ridge_penalty[0] = 0   # do not regularize bias term\n",
    "                        g = g + ridge_penalty\n",
    "                        \n",
    "                        # Update theta\n",
    "                        self.theta = self.theta - self.learning_rate * g\n",
    "        \n",
    "                print(\"\\n[Custom SGD + Ridge] Theta:\\n\", self.theta)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "         if self.use_sklearn:\n",
    "            return self.model.predict(X_test)\n",
    "         else:\n",
    "            return X_test.dot(self.theta)\n",
    "\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c988c0a-65fe-47b7-ba07-980c15ceda14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Custom SGD + Ridge] Theta:\n",
      " [[4.33314783]\n",
      " [2.33455116]]\n",
      "MSE (Custom SGD): 0.7779158115963563\n",
      "\n",
      "[scikit-learn Ridge/SGD] Theta:\n",
      " [[3.69632031]\n",
      " [3.05906335]]\n",
      "MSE (sklearn SGD): 0.7913645447724662\n"
     ]
    }
   ],
   "source": [
    "# Custom Ridge-SGD\n",
    "sgd_custom = SGDRegressor(learning_rate=0.01, epochs=30, lambda_reg=0.1, use_sklearn=False)\n",
    "sgd_custom.fit(X_train_b, y_train)   \n",
    "y_pred_custom = sgd_custom.predict(X_test_b)\n",
    "print(\"MSE (Custom SGD):\", sgd_custom.mse(y_test, y_pred_custom))\n",
    "\n",
    "# Scikit-learn Ridge-SGD\n",
    "sgd_sklearn = SGDRegressor(learning_rate=0.01, epochs=100, lambda_reg=0.1, use_sklearn=True)\n",
    "sgd_sklearn.fit(X_train, y_train)   \n",
    "y_pred_sklearn = sgd_sklearn.predict(X_test)\n",
    "print(\"MSE (sklearn SGD):\", sgd_sklearn.mse(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b171bb7-adc5-49a6-b482-2b36bf7bbe58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True theta:\n",
      " [[4]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(\"True theta:\\n\", true_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a65456f-1ce1-4343-8a30-876616d881f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "class MBGDRegressor:\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, epochs=100, batch_size=32, lambda_reg=0.1, alpha=0.5,use_sklearn=False):\n",
    "        self.learning_rate = learning_rate   \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda_reg = lambda_reg  \n",
    "        self.alpha = alpha            \n",
    "        self.use_sklearn = use_sklearn\n",
    "        self.theta = None\n",
    "        self.model = None  \n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        if self.use_sklearn:\n",
    "            # Use scikit-learn's ElasticNet\n",
    "            self.model = ElasticNet(\n",
    "                alpha=self.lambda_reg,\n",
    "                l1_ratio=self.alpha,     # balance L1/L2\n",
    "                max_iter=self.epochs * 10,\n",
    "                fit_intercept=True,\n",
    "                random_state=42\n",
    "            )\n",
    "            self.model.fit(X_train, y_train.ravel())\n",
    "            \n",
    "            # Combine intercept + coefficients into theta-like vector\n",
    "            self.theta = np.r_[self.model.intercept_, self.model.coef_].reshape(-1, 1)\n",
    "            print(\"\\n[scikit-learn ElasticNet] Theta:\\n\", self.theta)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            n_samples, n_features = X_train.shape\n",
    "            self.theta = np.random.randn(n_features, 1)\n",
    "    \n",
    "            for i in range(self.epochs):\n",
    "                # Shuffle data\n",
    "                indices = np.arange(n_samples)\n",
    "                np.random.shuffle(indices)\n",
    "                X_train = X_train[indices]\n",
    "                y_train = y_train[indices]\n",
    "    \n",
    "                # Process in mini-batches\n",
    "                for start_idx in range(0, n_samples, self.batch_size):\n",
    "                    end_idx = start_idx + self.batch_size\n",
    "                    X_batch = X_train[start_idx:end_idx]\n",
    "                    y_batch = y_train[start_idx:end_idx]\n",
    "    \n",
    "                    y_pred = X_batch.dot(self.theta)\n",
    "    \n",
    "                    # Gradient of MSE loss\n",
    "                    g = (2 / len(y_batch)) * X_batch.T.dot(y_pred - y_batch)\n",
    "    \n",
    "                    # Elastic Net penalty (exclude bias term at index 0)\n",
    "                    l1_penalty = self.alpha * self.lambda_reg * np.sign(self.theta)\n",
    "                    l2_penalty = (1 - self.alpha) * 2 * self.lambda_reg * self.theta\n",
    "                    penalty = l1_penalty + l2_penalty\n",
    "                    penalty[0] = 0   # do not regularize bias\n",
    "    \n",
    "                    g = g + penalty\n",
    "    \n",
    "                    # Update theta\n",
    "                    self.theta = self.theta - self.learning_rate * g\n",
    "    \n",
    "            print(\"\\n[Custom MBGD + ElasticNet] Theta:\\n\", self.theta)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        if self.use_sklearn:\n",
    "            return self.model.predict(X_test)\n",
    "        else:\n",
    "            return X_test.dot(self.theta)\n",
    "    def mse(self, y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "800b1a98-0a11-4087-a81c-34c25168e186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Custom MBGD + ElasticNet] Theta:\n",
      " [[4.15486262]\n",
      " [2.80536454]]\n",
      "MSE (Custom MBGD): 0.8420683147075902\n",
      "\n",
      "[scikit-learn ElasticNet] Theta:\n",
      " [[4.18677669]\n",
      " [2.7069881 ]]\n",
      "MSE (sklearn MBGD): 0.798525323786021\n"
     ]
    }
   ],
   "source": [
    "# Custom MBGD with ElasticNet\n",
    "mbgd_custom = MBGDRegressor(learning_rate=0.05, epochs=200, batch_size=16, lambda_reg=0.1, alpha=0.5, use_sklearn=False)\n",
    "mbgd_custom.fit(X_train_b, y_train)   \n",
    "y_pred_custom = mbgd_custom.predict(X_test_b)\n",
    "print(\"MSE (Custom MBGD):\", mbgd_custom.mse(y_test, y_pred_custom))\n",
    "\n",
    "# Scikit-learn ElasticNet\n",
    "mbgd_sklearn = MBGDRegressor(lambda_reg=0.1, alpha=0.5, use_sklearn=True)\n",
    "mbgd_sklearn.fit(X_train, y_train)   \n",
    "y_pred_sklearn = mbgd_sklearn.predict(X_test)\n",
    "print(\"MSE (sklearn MBGD):\", mbgd_sklearn.mse(y_test, y_pred_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163a372-d2e4-4b30-9de6-9637222d328b",
   "metadata": {},
   "source": [
    "**Batch Gradient Descant, Stochastic Gradient Descant and Mini Batch Gradient Descant without Regularization using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bbd62fb-462f-4c27-a76d-6f034d1b6031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 or 2:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Option: 2\n",
      "X shape: (100, 3)\n",
      "y shape: (100, 1)\n",
      "True Theta:\n",
      " [[4]\n",
      " [3]\n",
      " [3]\n",
      " [3]]\n",
      "\n",
      "Batch GD Theta:\n",
      " [[3.62554599]\n",
      " [3.23413919]\n",
      " [3.26765292]\n",
      " [3.02172852]]\n",
      "MSE - BGD (Test): 1.170324872471954\n",
      "\n",
      "Stochastic GD Theta:\n",
      " [[3.4040081 ]\n",
      " [3.29356037]\n",
      " [3.37127471]\n",
      " [3.13050345]]\n",
      "MSE - SGD (Test): 1.2335027523648003\n",
      "\n",
      "Mini-Batch GD Theta:\n",
      " [[3.6331844 ]\n",
      " [3.26085041]\n",
      " [3.270325  ]\n",
      " [3.03619596]]\n",
      "MSE - MBGD (Test): 1.170125208493768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Dataset creation ---\n",
    "def switch_example(value):\n",
    "    if value == 1:\n",
    "        X = 2 * np.random.rand(100, 1)\n",
    "        true_theta_simple = np.array([4, 3]).reshape(-1, 1)\n",
    "        y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "        return X, y, true_theta_simple\n",
    "    elif value == 2:\n",
    "        X = 2 * np.random.rand(100, 3)\n",
    "        true_theta_multiple = np.array([4, 3, 3, 3]).reshape(-1, 1)\n",
    "        y = 4 + 3*X[:,0] + 3*X[:,1] + 3*X[:,2] + np.random.randn(100)\n",
    "        y = y.reshape(-1, 1)\n",
    "        return X, y, true_theta_multiple\n",
    "    else:\n",
    "        return \"Invalid choice. Please select 1 or 2.\"\n",
    "\n",
    "# --- User input ---\n",
    "choice = int(input(\"Enter 1 or 2: \"))\n",
    "result = switch_example(choice)\n",
    "\n",
    "if isinstance(result, tuple):\n",
    "    X, y, true_theta = result\n",
    "    print(\"\\nSelected Option:\", choice)\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    print(\"True Theta:\\n\", true_theta)\n",
    "else:\n",
    "    print(result)\n",
    "    exit()\n",
    "\n",
    "# -----------------------------\n",
    "# Split into training and testing\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Add bias term for Batch GD\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. Batch Gradient Descent\n",
    "# --------------------------------------\n",
    "def batch_gradient_descent(X, y, lr=0.01, n_iterations=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.random.randn(n, 1)\n",
    "    for _ in range(n_iterations):\n",
    "        gradients = 2/m * X.T.dot(X.dot(theta) - y)\n",
    "        theta -= lr * gradients\n",
    "    return theta\n",
    "\n",
    "theta_bgd = batch_gradient_descent(X_train_b, y_train)\n",
    "y_pred_bgd = X_test_b.dot(theta_bgd)\n",
    "print(\"\\nBatch GD Theta:\\n\", theta_bgd)\n",
    "print(\"MSE - BGD (Test):\", mean_squared_error(y_test, y_pred_bgd))\n",
    "\n",
    "# ----------------------------------------------\n",
    "# 2. Stochastic Gradient Descent using sklearn\n",
    "# ----------------------------------------------\n",
    "sgd_reg = SGDRegressor(max_iter=1000, learning_rate='constant', eta0=0.01,\n",
    "                       penalty=None, random_state=42)\n",
    "sgd_reg.fit(X_train, y_train.ravel())\n",
    "theta_sgd = np.r_[sgd_reg.intercept_, sgd_reg.coef_]\n",
    "y_pred_sgd = sgd_reg.predict(X_test)\n",
    "print(\"\\nStochastic GD Theta:\\n\", theta_sgd.reshape(-1,1))\n",
    "print(\"MSE - SGD (Test):\", mean_squared_error(y_test, y_pred_sgd))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Mini-Batch Gradient Descent using partial_fit\n",
    "# -------------------------------------------------\n",
    "batch_size = 20\n",
    "n_epochs = 50\n",
    "y_train_flat = y_train.ravel()\n",
    "\n",
    "mbgd_reg = SGDRegressor(max_iter=1, tol=None, learning_rate='constant', eta0=0.01,\n",
    "                        penalty=None, random_state=42, warm_start=True)\n",
    "\n",
    "m_train = X_train.shape[0]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    indices = np.random.permutation(m_train)\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train_flat[indices]\n",
    "    \n",
    "    for start in range(0, m_train, batch_size):\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        mbgd_reg.partial_fit(X_batch, y_batch)\n",
    "\n",
    "theta_mbgd = np.r_[mbgd_reg.intercept_, mbgd_reg.coef_]\n",
    "y_pred_mbgd = mbgd_reg.predict(X_test)\n",
    "print(\"\\nMini-Batch GD Theta:\\n\", theta_mbgd.reshape(-1,1))\n",
    "print(\"MSE - MBGD (Test):\", mean_squared_error(y_test, y_pred_mbgd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da782f3a-cc2a-49bd-a137-cc218c242b5e",
   "metadata": {},
   "source": [
    "![alt text](Table-1-Using-Sklearn.png \"Using Sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b6fae-58cc-4939-823d-99800a660ab2",
   "metadata": {},
   "source": [
    "![alt text](Table-1-Graph-using-sklearn.png \"Simple Linear Regression and Multiple Linear Regression with MSE values for each types of GD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aa70e6-18ed-4895-9a0d-4ee81af5de7d",
   "metadata": {},
   "source": [
    "![alt text](Table-2-without-sklearn.png \"Table 2 Without Sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdaec7-adf2-4821-9f99-f5f714a1e5c1",
   "metadata": {},
   "source": [
    "![alt text](Table-2-Graph-without-using-sklearn.png \"Simple Linear Regression and Multiple Linear Regression with MSE and type of GD\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03d0645b-2431-429d-9a57-f7b6e335c44a",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "The experimental comparison of gradient descent approaches for both simple and multiple linear regression highlights the critical role of regularization in improving model performance and stability. \n",
    "\n",
    "For simple linear regression (true θ = [4, 3]), results without regularization showed biased parameter estimates and higher prediction errors, with Mean Squared Error (MSE) values typically above 1.3. Incorporating regularization consistently reduced MSE to below 0.9, while also producing parameter estimates much closer to the true values. Stochastic Gradient Descent with regularization achieved the lowest error, demonstrating its robustness. \n",
    "\n",
    "For multiple linear regression (true θ = [4, 3, 3, 3]), the effect of regularization was even more pronounced. Without regularization, the estimated coefficients deviated significantly from the true values, and MSE often exceeded 1.5, indicating both bias and overfitting. With regularization, however, parameter estimates became more stable, extreme coefficient variations were controlled, and MSE was consistently reduced to the range of 0.60–0.77. This confirms that regularization effectively prevents overfitting and improves generalization in higher-dimensional problems. \n",
    "    \n",
    "Across all gradient descent variants (batch, stochastic, and mini-batch), regularization not only reduced error by nearly 40–60% but also aligned the estimated parameters more closely with the true θ values. Moreover, it provided stability against the fluctuations that otherwise occurred in unregularized stochastic and mini-batch approaches. \n",
    "    \n",
    "Regularization enhances the effectiveness of gradient descent by penalizing overly large weights, thereby reducing error, improving convergence, and ensuring more reliable parameter estimation. Its importance grows with model complexity, making it an indispensable tool for regression tasks in both theoretical and practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548221d5-e0bd-40a9-a2dd-1f7241e842e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86f436-c0ee-42f0-a4a2-7aac7f30d6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
